Voici un fichier `README.md` clair, structurÃ© et professionnel, rÃ©sumant tout ce que tu as mis en place jusqu'Ã  l'ingestion des donnÃ©es brutes EHR dans Snowflake :

---

```markdown
# ğŸ“¦ EHR Data Pipeline â€“ Ingestion vers Snowflake (RAW Layer)

Ce projet de Data Engineering vise Ã  crÃ©er un pipeline analytique autour de donnÃ©es mÃ©dicales (EHR â€“ Electronic Health Records). Cette premiÃ¨re Ã©tape dÃ©crit le chargement des donnÃ©es brutes depuis des fichiers CSV locaux vers Snowflake dans un schÃ©ma RAW.

---

## ğŸ—ï¸ Structure du projet

```

ehr\_pipeline/
â”œâ”€â”€ data/                            # Fichiers CSV sources (EHR)
â”‚   â”œâ”€â”€ training\_SyncAllergy.csv
â”‚   â”œâ”€â”€ training\_SyncDiagnosis.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ ingest\_raw\_data.sql         # Script SQL d'ingestion complet
â”œâ”€â”€ dbt/                            # Ã€ venir : Transformation avec dbt
â”‚   â””â”€â”€ ehr\_pipeline/
â”‚       â””â”€â”€ models/
â””â”€â”€ README.md

````

---

## ğŸ§° Outils utilisÃ©s

- **Snowflake** : entrepÃ´t de donnÃ©es cloud (stockage + calcul)
- **SnowSQL** : client en ligne de commande pour interagir avec Snowflake
- **CSV** : fichiers source stockÃ©s localement
- **SQL** : script complet d'automatisation

---

## ğŸš€ Ã‰tapes de l'ingestion

### 1. ğŸ”§ Configuration initiale

- Base : `ehr_pipeline`
- SchÃ©ma : `RAW`
- RÃ´le : `ACCOUNTADMIN` (ou autre avec privilÃ¨ges suffisants)

```sql
USE ROLE ACCOUNTADMIN;
USE DATABASE ehr_pipeline;
USE SCHEMA RAW;
````

---

### 2. ğŸ—ƒï¸ CrÃ©ation du stage

CrÃ©ation dâ€™un stage nommÃ© `ehr_stage` avec configuration adaptÃ©e aux fichiers CSV (dÃ©limiteurs, en-tÃªtes, valeurs NULL, etc.)

```sql
CREATE OR REPLACE STAGE ehr_stage 
FILE_FORMAT = (
    TYPE = CSV 
    FIELD_DELIMITER = ','
    FIELD_OPTIONALLY_ENCLOSED_BY = '"'
    SKIP_HEADER = 1
    TRIM_SPACE = TRUE
    NULL_IF = ('NULL', 'null', '', 'N/A')
    EMPTY_FIELD_AS_NULL = TRUE
    ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE
);
```

---

### 3. ğŸ“¤ Upload des fichiers locaux (PUT)

Ã€ exÃ©cuter depuis le terminal **SnowSQL**, cette Ã©tape charge les fichiers CSV vers le stage Snowflake :

```sql
PUT file://ehr_pipeline/data/training_SyncAllergy.csv @ehr_stage AUTO_COMPRESS=FALSE;
-- RÃ©pÃ©tÃ© pour les 16 fichiers
```

---

### 4. ğŸ“¥ Chargement dans les tables RAW

Chaque fichier est copiÃ© vers sa table cible avec gestion des erreurs :

```sql
COPY INTO RAW.ALLERGY 
FROM @ehr_stage/training_SyncAllergy.csv 
ON_ERROR = CONTINUE;
-- RÃ©pÃ©tÃ© pour chaque fichier
```

---

### 5. ğŸ“Š VÃ©rification du chargement

RequÃªte finale pour valider le nombre de lignes chargÃ©es :

```sql
SELECT 
    table_name,
    row_count,
    CASE 
        WHEN row_count = 0 THEN 'âš ï¸  EMPTY'
        WHEN row_count > 0 THEN 'âœ… LOADED'
    END AS status
FROM (
    SELECT 'ALLERGY', COUNT(*) FROM RAW.ALLERGY UNION ALL
    ...
);
```

---

## âœ… RÃ©sultat attendu

Ã€ la fin de l'ingestion, toutes les donnÃ©es brutes seront disponibles dans le schÃ©ma `RAW` de la base `ehr_pipeline`, prÃªtes Ã  Ãªtre transformÃ©es via **dbt** (`stg_`, `dim_`, `fact_`).

---

## ğŸ”œ Prochaine Ã©tape

CrÃ©er les modÃ¨les de **staging (`stg_`)** dans dbt Ã  partir des tables RAW pour amorcer la couche de transformation.

---

## ğŸ‘¨â€ğŸ’» Auteur

Khalifa Ababacar Seck
Projet acadÃ©mique â€“ Master en Informatique (Data Engineering)
UniversitÃ© de Sherbrooke, 2025

```

---

Souhaites-tu que je gÃ©nÃ¨re aussi le `README.md` pour les Ã©tapes dbt Ã  venir (staging, marts, facts, etc.) ?
```
