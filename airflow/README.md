# üõ†Ô∏è Orchestration Airflow

> **Pr√©-requis** : Copier le dossier `dbt/ehr_pipeline` dans `dags/dbt/ehr_pipeline` du conteneur Airflow pour garantir que le projet dbt est accessible aux DAGs.

L‚Äôorchestration est assur√©e par **Apache Airflow** avec l‚Äôop√©rateur Cosmos :

* **dbt\_run\_staging** : ex√©cute les mod√®les de nettoyage dans le sch√©ma `STAGING`.
* **dbt\_run\_dimensions** : ex√©cute les mod√®les de dimensions dans le sch√©ma `MARTS`.
* **dbt\_run\_facts** : ex√©cute les mod√®les de faits dans le sch√©ma `MARTS`.
* **dbt\_test** : lance les tests dbt sur tous les mod√®les d√©ploy√©s.

---

## üõ†Ô∏è 4.1 √âtapes d‚Äôimpl√©mentation du DAG Airflow

1. **D√©finition des connexions**

   * Cr√©er une connexion Snowflake (`conn_ehr_pipeline`) dans l‚ÄôUI Airflow.
   * Configurer host, login, mot de passe, entrep√¥t et r√¥le via les variables ou secrets Airflow.

2. **Configuration du ProfileConfig**

   * Utiliser `ProfileConfig` et `SnowflakeUserPasswordProfileMapping` pour pointer vers la connexion Airflow.
   * Sp√©cifier dynamiquement `database`, `schema`, `role` et `account`.

3. **Construction du DAG**

   * Instancier un DAG Airflow avec `schedule='@daily'`, `catchup=False` et tags adapt√©s.
   * D√©finir quatre t√¢ches : `DbtRunLocalOperator` pour `staging`, `dimensions`, `facts`, et `DbtTestLocalOperator`.

4. **Gestion des d√©pendances**

   * Cha√Æner les op√©rateurs dans l‚Äôordre : staging ‚Üí dimensions ‚Üí faits ‚Üí tests.
   * S‚Äôassurer que `install_deps=True` sur la premi√®re t√¢che pour installer les packages dbt.

5. **Monitoring et alerting**

   * Configurer des alertes (e-mail ou Slack) en cas d‚Äô√©chec de t√¢che via les callbacks Airflow.
   * Consulter les logs et XComs pour diagnostiquer les erreurs dbt.

6. **Test et d√©ploiement**

   * Valider localement avec la CLI Airflow (`airflow tasks test`).
   * D√©ployer le DAG sur le serveur Airflow et surveiller les ex√©cutions planifi√©es.
