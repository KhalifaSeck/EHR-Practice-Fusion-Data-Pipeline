# ğŸ“¦ EHR Practice Fusion Data Pipeline

Ce projet met en place un **pipeline complet** pour exploiter des donnÃ©es EHR (Electronic Health Records) de Practice Fusion dans Snowflake, les transformer avec **dbt**, puis orchestrer le tout avec **Airflow**.

---

## ğŸ¯ Objectifs du projet

* Mettre en place un processus automatisÃ© d'ingestion des donnÃ©es EHR brutes dans Snowflake.
* Standardiser et transformer ces donnÃ©es via des modÃ¨les dbt (staging, dimensions, faits).
* Orchestrer lâ€™ensemble avec Apache Airflow pour garantir un pipeline fiable et rÃ©pÃ©table.

---

**Source des donnÃ©es** : [Practice Fusion EHR Dataset sur Kaggle](https://www.kaggle.com/c/pf2012)

---

## ğŸš€ 1. Ingestion des donnÃ©es brutes (RAW)

1. **CrÃ©ation du stage CSV**

   * Dans Snowflake (schÃ©ma **RAW**) :

     ```sql
     CREATE OR REPLACE STAGE ehr_stage
       FILE_FORMAT = (
         TYPE = CSV,
         FIELD_DELIMITER = ',',
         FIELD_OPTIONALLY_ENCLOSED_BY = '"',
         SKIP_HEADER = 1,
         TRIM_SPACE = TRUE,
         NULL_IF = ('NULL','null','','N/A'),
         EMPTY_FIELD_AS_NULL = TRUE,
         ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE
       );
     ```

2. **Upload des fichiers**

   ```bash
   snowsql -c ehr_pipeline -q "PUT file://./data/*.csv @ehr_stage AUTO_COMPRESS=FALSE;"
   ```

3. **Chargement en base**

   * ExÃ©cuter : `ingestion/ingest_raw_data.sql` contenant :

     ```sql
     COPY INTO RAW.ALLERGY            FROM @ehr_stage/training_SyncAllergy.csv ON_ERROR=CONTINUE;
     COPY INTO RAW.DIAGNOSIS          FROM @ehr_stage/training_SyncDiagnosis.csv ON_ERROR=CONTINUE;
     COPY INTO RAW.PATIENT            FROM @ehr_stage/training_SyncPatient.csv ON_ERROR=CONTINUE;
     -- ... autres tables
     COPY INTO RAW.TRANSCRIPTMEDICATION FROM @ehr_stage/training_SyncTranscriptMedication.csv ON_ERROR=CONTINUE;
     ```

4. **VÃ©rification**

   * Un bloc SQL de `SELECT COUNT(*)` pour chaque table valide le chargement.

---

## ğŸ§± 2. ModÃ©lisation dbt en trois couches

### ğŸ¯ 2.1 Couche **STAGING** (schÃ©ma `STAGING`)

* **Objectif** : Normaliser et typer les donnÃ©es brutes.
* **ModÃ¨les** : `models/staging/stg_*.sql` (materialized as `view`).
* **Documentation & tests** : `models/staging/schema.yml`.

### ğŸ¯ 2.2 Couche **MARTS** â€“ Dimensions (schÃ©ma `MARTS`)

* **Objectif** : CrÃ©er les rÃ©fÃ©rentiels dÃ©-duppliquÃ©s (`dim_*`).
* **ModÃ¨les** : `models/marts/dim/dim_*.sql` (materialized as `table`).
* **Documentation & tests** : `models/marts/schema.yml`.

### ğŸ¯ 2.3 Couche **MARTS** â€“ Faits (schÃ©ma `MARTS`)

* **Objectif** : AgrÃ©ger et historiser les indicateurs (`fct_*`).
* **ModÃ¨les** : `models/marts/fact/fct_*.sql` (materialized as `table`).
* **Documentation & tests** : idem.

---

## âš™ï¸ 3. Configuration dbt

### 3.1 dbt\_project.yml

```yaml
models:
  ehr_pipeline:
    staging:
      +schema: STAGING
      +materialized: view

    marts:
      +schema: MARTS
      +materialized: table
      dim:
        +schema: MARTS
      fact:
        +schema: MARTS
```

### 3.2 profiles.yml

```yaml
ehr_pipeline:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: lw87791.ca-central-1.aws
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
      role: dbt_role
      database: EHR_PIPELINE
      warehouse: ehr_wh
      schema: STAGING
      threads: 4
```

> **NB** : Ne pas committer de credentials ; utilisez un fichier `.env` ou les secrets CI/CD.

---

## ğŸ› ï¸ 4. Orchestration Airflow

Lâ€™orchestration est assurÃ©e par **Apache Airflow** avec lâ€™opÃ©rateur Cosmos pour :

* **dbt\_run\_staging** : exÃ©cuter les modÃ¨les de nettoyage (`models/staging`).
* **dbt\_run\_dimensions** : exÃ©cuter les modÃ¨les de dimensions (`models/marts/dim`).
* **dbt\_run\_facts** : exÃ©cuter les modÃ¨les de faits (`models/marts/fact`).
* **dbt\_test** : lancer les tests dbt sur les modÃ¨les gÃ©nÃ©rÃ©s.

La connexion Ã  Snowflake utilise un **Connection ID** configurÃ© dans Airflow, qui pointe vers les variables dâ€™environnement et secrets (login, mot de passe, account, warehouse, role).

## ğŸ“Š 5. Flux de donnÃ©es et dÃ©pendances Flux de donnÃ©es et dÃ©pendances

```mermaid
graph TD
    A[CSV Raw] --> B[Snowflake RAW]
    B --> C[dbt Staging]
    C --> D[dbt Dimensions]
    C --> E[dbt Facts]
    D --> F[dbt Tests]
    E --> F
    subgraph Airflow
      G[dbt_run_staging] --> H[dbt_run_dimensions] --> I[dbt_run_facts] --> J[dbt_test]
    end
```

---
